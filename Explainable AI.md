[TOC]



# 1 入门

**1.传统算法和机器学习算法比较：**

- **传统算法：** 基于手写规则的程序，如能获得优惠计划的客户分类
- **机器学习算法：** 无需简单规则，基于多个输入数据自动学习，如贷款风险评估

**2.机器学习的基本成分与模型训练**

- **函数类:** 定义输出 (y) 与输入 (x) 的关系

- **训练数据:** 包含特征和标签的数据样本

**3.现代机器学习的黑盒问题**

- **黑盒模型：** 尽管模型的准确性与商业收入、用户体验密切相关，但缺乏透明度

- **透明性的重要性：** 需要识别关键因素并生成科学假设

**4.临床预测的关键问题：Why**

- **挑战：** 精确预测临床结果很重要，但核心问题在于理解预测背后的原因

**5.解释性AI在生物学和健康中的应用**

- **解释性的重要性：** 解释性有时与准确性同等重要，例如在临床应用中，了解特征对预测的影响至关重要

- **关键问题：**

  - 哪些特征对某一预测起到了关键作用？

  - 如何选择或学习最具解释性或信息量的特征？

  - 如何使黑盒模型具有生物学或临床意义？

**6.黑盒模型的透明性目标**

- 目标包括：
  - 识别潜在过程中的关键因素
  - 生成科学假设
  - 模型故障诊断和审查
  - 审核不必要的依赖项
  - 改善数据集和提高信任

**7.解释性AI的关键成分**

- **模型：** 可以是黑盒模型 (如DNN, GBM)
- **数据：** 单个数据样本或整个数据集
- **问题：** 需要了解什么方面？

**8.XAI 问题类型**

- **特征重要性**：哪些特征对预测最重要？
- **概念重要性**：模型理解的关键概念是什么？
- **数据实例重要性**：每个样本对模型训练的贡献程度

**9.临床预测中的准确性与解释性**

- 尽管准确预测临床结果很重要，但关键问题在于“为什么”模型做出这样的预测。
- **精确性与可解释性：** 简单模型（如线性模型）通常解释性强，但性能差；复杂模型（如黑盒模型）尽管准确，但难以解释。
- **SHAP方法：** Scott Lundberg 和 Su-In Lee提出了SHAP方法，用于任何模型的特征重要性计算，能解释单一预测的依据。
- 临床预测中的解释性提高了决策支持：使用XAI提高手术期间低氧血症的预测准确性，提供实时解释，帮助麻醉师识别潜在的低氧风险。

**10.解释性AI促进模型审核和成本意识**

- **模型审核的必要性：** 许多基于AI的COVID-19检测系统依赖于“捷径”特征而非真正的病理学依据，表明模型审核的重要性。

- **成本意识AI (`Cost-Aware AI`,CoAI)：** CoAI方法显著减少特征获取成本（如时间），可应用于急诊和重症监护。

  

# 2 什么是XAI

**1.解释性AI (XAI) 的定义**

- **狭义定义：** 让模型决策对人类更易理解的方法和技术
- **广义定义：** 让所有AI相关内容（如数据、功能、性能等）更易理解
- **重点：** 主要关注监督式机器学习的解释
- **权衡问题(Trade-Off Issue)：** 高性能模型（如深度神经网络）往往难以解释，直接可解释模型性能相对较低。
- **XAI重要性：** 解释性是负责任AI的基础，有助于实现公平性、可用性、安全性等目标。
  - **解释性调试：** XAI 有助于识别模型中的问题并通过调试改进模型。
  - Kulesza 等人提出的解释性调试方法，可以通过用户反馈不断优化模型

**2.监督式机器学习**

- **学习模型：** 使用算法训练模型，以预测新实例的标签。
- **特征解释：** XAI的重点是解释模型的决策过程，例如苹果蛋糕的颜色、形状和气味等特征。
- **模型解释：** 包括性能、限制、输出等。

**3.XAI类型**

- **直接解释模型**：如线性模型、决策树、基于规则的模型
- **事后解释模型**：针对黑盒模型，如深度神经网络，可用近似方法提供解释。

**案例：贷款申请审批中的决策支持**

- **参与角色：** 数据科学家、贷款官员、客户
- **需求：** 客户希望了解贷款审批的理由，贷款官员需要对预测结果有信心。

**4.模型的全局与局部解释**

- **Knowledge Distillation**：

  - 知识蒸馏是一种方法，通常用来将复杂模型（如深度神经网络）的知识“蒸馏”到一个更简单的模型中。

  - 在这里，知识蒸馏被用来将黑盒模型的行为通过近似方法转移到一个更易解释的模型上，例如决策树或线性模型，以便更好地理解原始模型的整体行为。

- **全局解释：** 如决策树近似，帮助数据科学家理解整体模型的决策逻辑。
- **局部解释：** 分析单个预测，如局部特征贡献和原型示例，帮助贷款官员做出判断。

**5.事后解释算法示例**

**LIME方法：** LIME 是一种用于解释黑盒模型（如深度神经网络和随机森林）的局部解释方法。它通过构建一个 **易解释的、线性的近似模型** 来模拟黑盒模型在特定输入附近的行为，进而揭示该输入如何影响模型的预测。

<font color=blue>**步骤：**</font>

**生成扰动数据**：

- LIME 生成与输入样本相似的扰动数据集（例如，通过小幅改变特征值），并将这些数据输入到原始黑盒模型中，记录模型的输出。

**权重分配**：

- 根据扰动数据点与原始输入样本的相似性，LIME 为这些数据点分配权重，确保越接近原始样本的扰动点在解释中权重越高。

**训练局部线性模型**：

- 使用带权重的扰动数据，LIME 训练一个简单的线性模型来近似原始模型在该局部区域的行为。

**生成解释**：

- LIME 的最终输出是每个特征对该预测结果的贡献度（正或负），以揭示哪些特征在多大程度上影响了该预测。

**6.反事实检查**

- **反事实特征检查：** 如部分依赖图和对比特征，展示不同决策条件下的特征影响。
- **反事实示例：** 分析类似实例，如其他按时还款的客户，帮助理解贷款被拒的原因。

**7.XAI评价指标**

- **固有指标：** 如保真度、稳定性、简洁性
- **用户依赖指标：** 如可理解性、用户满意度
- **任务导向指标：** 任务表现和用户信任度

**8.不想要的偏差(unwanted bias)**

- **偏差问题：** 偏差可能会导致某些非特权群体处于系统性不利地位，在某些背景下甚至是非法的。
  - **背景：** COMPAS 系统用于预测犯罪率，但被发现对某些群体存在歧视性偏见。

**XAI审查**：

- **作用：** XAI 可以作为一种工具，用于分析和揭示模型中的歧视性偏见。

- **方法：** 通过对比性示例和特征重要性分析帮助用户理解模型决策

**9.XAI优点**

- **重要性：** XAI 提供的解释有助于用户理解决策背后的原因，从而采取合适的行动。
- **信任校准：** XAI 可以帮助校准人们对 AI 的信任，增加用户对系统的依赖度。
- **AI 文档和治理的趋势：** AI 的文件记录和问责机制变得越来越重要，像 IBM 的 FactSheets 和 Google 的 Model Cards 等工具已经被提出。
  - **目标：** 这些工具有助于在 AI 系统中实现透明性和可追溯性，保障 AI 的安全性和公正性

# 3 解释方法

## 3.1 特征重要性解释(Feature Importance Explanations)

**目的**：解释每个特征如何影响模型，并回答模型决策相关的问题。

**例子**：

- 包括自然图像、医疗图像、表格数据和时间序列等场景。
- 研究例子：Fong 和 Vedaldi 的研究“通过有意义的扰动解释黑盒模型”
  - 通过对图像进行有意义的扰动（例如遮盖或模糊特定区域），观察模型的预测变化。该方法旨在发现哪些图像区域对于模型预测最重要。
- 研究例子：Sundararajan 等人提出的“深度网络的公理化归因”
  - 提出了一种基于深度神经网络的归因方法，称为“积分梯度法”（Integrated Gradients），它利用数学公理来确定每个像素对最终预测的贡献。这种方法在不改变模型结构的情况下生成解释性信息。
- 研究例子：Lundberg 等人关于“树的解释性 AI：从局部解释到全局理解”
  - 提出了 SHAP（Shapley Additive Explanations）方法，基于博弈论中的 Shapley 值来计算每个特征在特定预测中的边际贡献。该方法可以为树模型提供局部和全局的解释。
- 研究例子：Covert 等人关于“用于自动癫痫检测的时间图卷积网络”（2019年）
  - 提出了时间图卷积网络（Temporal Graph Convolutional Networks），在时间序列中检测出癫痫发作等重要事件，并解释模型如何识别时间序列中的关键模式。

### 3.1.1 符号说明

常用的符号：

1. **𝑥**：输入数据或特征向量

   这是模型接受的输入，表示一个具体的数据实例。𝑥 包含了多个特征的值。

2. **𝑦**：模型的输出或预测结果

   𝑦 是模型对输入𝑥的预测输出，例如在分类问题中，𝑦 可能是一个类别标签。

3. **𝑓(𝑥)**：机器学习模型

   表示机器学习模型，它根据输入𝑥 生成输出𝑦。𝑓(𝑥) 的结构和算法因模型类型而异，如神经网络、决策树等。

4. **𝑥₁, 𝑥₂, ..., 𝑥𝑑**：特征（Features）

   每个输入𝑥包含𝑑个特征（feature），即𝑥是一个包含𝑑个维度的向量。例如，𝑥₁可以代表年龄，𝑥₂可以代表收入。

5. **𝑑**：特征的总数

   表示输入数据中所包含的特征数量。

6. **𝑛**：样本数

   数据集中数据点的总数，即样本数。𝑛个数据点表示模型接受了多少个独立的输入数据实例进行训练或测试。

### 3.1.2 定义

**模型解释（Model Explanation）**

模型解释的目标是突出模型为什么会做出某个预测。通过揭示模型在做出特定决策时的原因，帮助用户理解模型的行为。

**特征重要性解释（Feature Importance Explanation）**

- 特征重要性解释专注于每个特征的作用，展示各个特征对模型预测的影响大小。
- 这种解释可以是针对单个预测的（局部解释，Local）或针对模型整体行为的（全局解释，Global）。

**局部解释与全局解释**

- **局部解释（Local）**：局部解释关注单个数据实例的特征重要性，展示某个特定预测的原因。
- **全局解释（Global）**：全局解释旨在描述模型整体的行为，展示模型在所有数据点上的特征重要性。

**特征重要性解释的两种类型**

- **特征归因（Feature Attribution）**：对每个特征 $$ x_i $$ 赋予一个分数 $$ a_i $$，这个分数表示该特征对模型预测的贡献。
- **特征选择（Feature Selection）**：从所有特征中选择一个重要特征子集$$ x_S \subset \{x_1, x_2, \dots, x_d\} $$，这些特征在预测中起到关键作用。

**解释算法（Explanation Algorithm）**

解释算法是一种基于输入数据和机器学习模型生成解释的方法。它的目的是使模型的预测过程对用户更加透明。



## 3.2 基于移除的解释(Removal-based Explanations)

- **基本思想**：移除特征并观察预测变化，判断特征重要性。

- **医生类比与机器学习转化**
  - 通过遮挡医疗图像的部分区域，观察诊断变化，类似于 ML 中移除特征。

### 3.2.1 案例研究

#### 3.2.1.1 置换测试(permutation test)

置换测试是一种用于评估输入特征重要性的“老方法”，最早在随机森林模型中应用。通过该方法，可以确定每个输入特征的总体（全局）重要性。

- **步骤 1**：首先使用原始数据评估模型的准确性。
- **步骤 2**：逐个特征进行破坏，即随机打乱数据集中特定特征列的值（对应于该特征）。
- **步骤 3**：记录模型准确性的下降幅度，以此来衡量特征的重要性。

该方法通过对数据集中的每个特征进行随机扰动，观察其对预测精度的影响，从而评估各个特征的重要性。这就是置换测试的基本原理。

- 置换测试适用于任何模型
- 可用于连续特征或类别特征
- 快速且易于实现

**数学定义**

(1)直观的数学定义
$$
a_i = Acc(\text{original}) - Acc(x_i \text{ corrupted})
$$

- **$$ a_i $$**：特征 $$ x_i $$ 的重要性分数，表示当特征 $$ x_i $$ 被破坏或置换时，模型准确性的下降程度。
- **$$ Acc(\text{original}) $$**：模型在使用完整原始数据时的准确性。
- **$$ Acc(x_i \text{ corrupted}) $$**：当特征 $$ x_i $$ 被随机打乱或破坏后，模型的准确性。

(2)详细视角的数学定义
$$
a_i = \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., \tilde{x}_i^j, ..., x_d^j), y^j) - \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., x_i^j, ..., x_d^j), y^j)
$$

- **$$ a_i $$**：特征 $$ x_i $$ 的重要性分数，衡量当特征 $$ x_i $$ 被扰乱后，对模型损失的平均影响。
- **$$ n $$**：样本数量。
- **$$ f(x_1^j, ..., x_d^j) $$**：模型 $$ f $$ 的输出，基于输入特征 $$ x_1^j, ..., x_d^j $$。
- **$$ \ell $$**：任意损失函数（例如均方误差或交叉熵），用于衡量模型预测值和真实值 $$ y^j $$ 之间的误差。
- **$$ \tilde{x}_i^j $$**：特征 $$ x_i $$ 在第 $$ j $$ 个样本中的扰乱版本（例如被随机打乱的值）。
- **第一部分** $$ \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., \tilde{x}_i^j, ..., x_d^j), y^j) $$：表示破坏特征 $$ x_i $$ 后的平均损失。
- **第二部分** $$ \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., x_i^j, ..., x_d^j), y^j) $$：表示原始数据下的平均损失（即特征未被破坏的情况）。



#### 3.2.1.2 遮挡(Occlusion)

遮挡方法是一种早期用于深度学习模型的解释技术，主要用于图像分类任务中的个别预测解释。

- **遮挡方法适用于任何模型**：不仅限于图像数据，还可用于非图像数据的模型解释。
- **速度适中**：对每个预测进行解释时，需要进行 d+1d + 1d+1 次模型评估（其中 ddd 为特征数）。
- **简单易实现**：遮挡方法实现简便，适合各种应用场景。

**方法概述**

- **深度学习的早期方法**：遮挡方法在深度学习刚兴起时被提出，作为理解复杂图像分类模型的技术。
- **用于解释单个预测**：主要关注于解释模型如何对单个图像做出预测，帮助理解模型为何对特定输入图像做出特定预测。
- **计算像素（或超像素）的重要性**：通过逐步遮挡图像中的某些像素或超像素区域，观察模型预测结果的变化，从而计算这些区域对预测结果的重要性。

遮挡方法的具体过程如下：

1. **使用完整图像进行预测**：首先，模型在未遮挡的完整图像上进行预测，得到基线预测结果。
2. **遮挡图像的不同区域**：将图像的不同区域逐一遮挡，并记录每次遮挡后预测结果的变化。这个过程帮助识别出哪些区域对模型预测最为关键。
3. **用无信息（如零）像素替换遮挡区域**：遮挡通常通过将像素替换为无信息的值（如零）来实现，这样可以保证该区域对预测不再有任何贡献。
4. **遮挡的粒度**：遮挡操作可以在不同的粒度上进行，例如 2x2 或 4x4 的超像素（superpixel）块。较小的遮挡块可以获得更精细的解释，而较大的遮挡块则有助于快速识别重要区域。

**数学定义**

(1)直观视角的数学定义
$$
a_i = f_y(x) - f_y(x_{(-i)})
$$

- **$$ a_i $$**：特征 $$ a_i $$ 的重要性分数，通过计算完整输入 $$ x $$ 的模型输出 $$ f_y(x) $$ 与去掉第 $$ i $$ 个特征后的模型输出 $$ f_y(x_{(-i)}) $$ 之间的差值来得出。
- **目的**：通过遮挡特定特征，观察模型输出的变化，从而量化该特征对模型预测的重要性。

(2)详细视角的数学定义
$$
a_i = f_y(x_1, ..., x_d) - f_y(x_1, ..., 0, ..., x_d)
$$

- **$$ a_i $$**：特征 $$ a_i $$ 的重要性分数，计算为完整输入 $$ (x_1, ..., x_d) $$ 的模型输出与将第 $$ i $$ 个特征替换为零后的模型输出之间的差值。
- **替换为零**：在详细视角中，遮挡操作通过将特征 $$ x_i $$ 替换为零（或其他无信息的值）实现，使其对模型预测不再产生影响。

#### 3.2.1.3 置换和遮挡方式对比

- **为不同模型设计**：置换适用于随机森林，遮挡适用于卷积神经网络（CNN）

- **全局与局部解释**：置换适用于全局解释，遮挡适用于局部解释。

- **相似之处**：尽管方法不同，但这些方法之间存在一些显著的相似之处。

|              | 置换测试         | 遮挡           |
| ------------ | ---------------- | -------------- |
| 破坏输入     | 随机化特征       | 设为零         |
| 观察模型变化 | 观察准确性的变化 | 观察预测的变化 |
| 计算影响     | 移除单个特征     | 移除单个特征   |

### 3.2.2 基于移除解释方法的统一框架

想法：通过更改实现选项来创建新的解释方法。

**三种设计选择**：**特征移除方法**、**模型行为**和**汇总技术**

<font color = blue>**特征移除**</font>

- **核心思想**：模型通常需要所有特征才能进行预测，但我们希望从某些特征中移除信息。
- **特征移除的模拟**：大多数模型不支持直接移除特征，因此需要模拟特征移除。
- **实现方式**：
  - **使用默认值（如零）替换**：将要移除的特征替换为零值。
  - **使用随机值替换**：用随机值替换特征内容。
  - **为每个特征集训练单独的模型**：构建包含不同特征的独立模型。
  - **使用支持缺失特征的模型**：选择可以接受缺失特征的模型。
  - **模糊处理（针对图像）**：对图像中的区域进行模糊处理，模拟特征移除。

<font color = blue>**模型行为**</font>

- **观察模型行为**：可以移除特征并观察其对模型的影响。

- 选择观察的量

  需要选择一个具体的量来评估模型行为，例如预测值、预测损失或数据集损失。

  - **Prediction（预测）**：直接观察预测结果。
  - **Prediction loss（预测损失）**：观察预测结果的损失值。
  - **Dataset loss（数据集损失）**：整体数据集上的损失值 $$ E[\ell (y, \hat{y})] $$。

<font color = blue>**汇总技术**</font>

- **选择汇总方法**：可以使用任意特征子集来观察模型行为。
- **组合过多问题**：给定 d 个特征，有 $$ 2^d $$ 个子集需要考虑。如何有效地传达信息是一个挑战。
- **常见的汇总类型**：
  - **Feature selection（特征选择）**：选取一组重要特征的子集。
  - **Feature attribution（特征归因）**：分配特征分数，例如使用置换测试和遮挡法。

----------

<font color=blue>**常用特征移除方法**</font>

- **PredDiff**：通过条件期望生成解释和交互，使用条件填充模型来删除信息
  - 条件删除模型会删除某些特征，但通过填充与邻近值或上下文相关的内容，尽可能保持数据的“合理性”或“自然性”。
- **Meaningful Perturbations**：考虑多种从输入图像中删除信息的方法，推荐的操作是模糊化

<font color=blue>**常用汇总方法**</font>

**(1)RISE**：对缺失特征的多个子集进行采样，计算包含 $$ x_i $$ 时的平均预测

**(2)LIME**：对特征子集应用加权核 $$ \pi(S) $$，拟合线性/加性代理模型
$$
\min_{a_0, ..., a_d} \sum_{S \subseteq D} \pi(S) \left( a_0 + \sum_{i \in S} a_i - f_y(x_S) \right)^2 + \Omega(a_1, ..., a_d)
$$

- **目标**：LIME 通过拟合一个 **加性近似模型**，来解释复杂模型的预测。LIME 的目标是找到一组系数 $$ a_0, a_1, ..., a_d $$，使得这些系数尽可能地描述目标模型在局部区域的行为。

- **加性近似（Additive Approximation）**：公式的核心部分 $$ a_0 + \sum_{i \in S} a_i $$ 表示一个线性模型的近似，其中 $$ a_0 $$ 是偏置项，$$ a_i $$ 是每个特征的贡献分数。

- **权重函数 $$ \pi(S) $$**：用于衡量不同特征子集的重要性，通常根据特征子集与被解释数据点的相似性进行加权，确保越接近原始实例的特征组合在解释中权重越高。

- **正则化项 $$ \Omega(a_1, ..., a_d) $$**：这是一个可选的正则化项（如 Lasso），用于控制系数的稀疏性，帮助简化模型，使解释更易于理解。

**工作流程部分**

1. **模型输入和预测**：

   复杂模型（例如神经网络）接受数据输入并生成预测结果。这里的数据包括了多个特征，例如打喷嚏、体重、头痛等，模型预测疾病为“流感”（Flu）。

2. **解释器（LIME）**：

   LIME 解释器接收原始数据和模型预测，选择一组与预测相关的重要特征，通过线性近似来解释模型的局部行为。LIME 提取了特征“打喷嚏”、“头痛”和“无疲劳”作为解释特征，用来简化复杂模型的输出。

3. **人类决策**：

   最终，解释器生成的解释（如“打喷嚏”、“头痛”、“无疲劳”）被展示给用户，人类基于这些解释做出进一步的判断或决策。

### 3.2.3 Meaningful perturbations(扩展学习)

- **目的**：从模型正确分类的图像开始，通过对图像进行模糊处理来改变预测结果。

- **过程**：通过模糊操作逐渐改变图像，使得模型的预测结果发生变化。
- **示例**：对比模糊图像和原始图像，展示了模糊如何影响模型的分类准确性。

<font color = blue>**步骤**</font>

- 设 $$ x \in \mathbb{R}^{w \times h} $$ 为图像
- 设 $$ m \in [0, 1]^{w \times h} $$ 为遮罩
- 设 $$ \Phi(x, m) $$ 为遮罩图像

**(1)问题**：如何进行遮罩操作？

- **使用常量值替换**：

$$
\Phi(x, m)_{ij} = m_{ij} \cdot x_{ij} + (1 - m_{ij}) \cdot \mu
$$

- **使用噪声替换**：

$$
\Phi(x, m)_{ij} = m_{ij} \cdot x_{ij} + (1 - m_{ij}) \cdot \epsilon_{ij}
$$

其中 $$ \epsilon \sim \mathcal{N}(0, \sigma^2) $$

- **使用高斯核模糊**：

$$
\Phi(x, m)_{ij} = \text{blur with kernel } g_{\sigma}
$$

**(2)学习最优模糊**

- **初始状态**：目标类别 $$ y $$ 对 $$ \Phi(x, 1) $$ 的预测概率接近 1

- **目标**：学习一个遮罩 $$ m $$，使得 $$ f_y(\Phi(x, m)) \approx 0 $$

- **最小化以下损失**：

  $$
  \min_m f_y(\Phi(x, m))
  $$

**(3)其他考虑因素**

1. 模糊应尽量最小
2. 遮罩应平滑。
3. 优化应对抗对抗性扰动。

**实际损失函数**
$$
\min_m \mathbb{E}_{x} [f_y(\Phi(x, m))] + \lambda_1 \|1 - m\|_1 + \lambda_2 \| \nabla m \|_2^2
$$

- 损失项
  - $$ f_y(\Phi(x, m)) $$：预测误差项
  - $$ \|1 - m\|_1 $$：稀疏遮罩正则化
  - $$ \| \nabla m \|_2^2 $$：平滑正则化

**(4)优化**

- 实际损失函数：
  $$
  L(m) = \mathbb{E}_{x}[f_y(\Phi(x, m))] + \lambda_1 \|1 - m\|_1 + \lambda_2 \| \nabla m \|_2^2
  $$

- 通过随机梯度下降（SGD）确定最优遮罩：

  $$
  m^{(t+1)} = m^{(t)} - \alpha \frac{\partial L}{\partial m^{(t)}}
  $$

**(5)结果**

- **不同模糊处理效果的比较**：展示了在模糊、常量替换和噪声扰动下的遮罩效果。
- **图像示例**：从不同的遮罩和扰动类型中可以看出，模糊处理能够有效地影响模型对目标的注意区域





## 3.3 Shapley值

### 3.3.1 背景

**SHAP值的定义**

Shapley 值是一种用于解释机器学习模型的方法，通过量化每个特征对预测结果的贡献，确保每个特征的影响被公平评估。这种方法基于合作博弈理论中的 Shapley 值，解决了复杂模型中特征贡献分配的问题。

**SHAP值的计算过程**

- 特征团队：模型的输入由多个特征构成，例如年龄、性别、BMI等。这些特征的组合影响最终的预测结果。
- 边际贡献：SHAP值通过计算每个特征在不同特征组合中的边际贡献，评估该特征对预测的平均影响。这个过程涉及计算特征在所有可能组合下的增量贡献。
- 公平分配：通过考虑所有可能的特征组合，SHAP值确保每个特征的贡献得到公平分配，避免单个特征被高估或低估。

**SHAP值的影响因素**

- **特征组合**：特征的边际贡献因组合的不同而变化。SHAP值计算时会综合所有可能的组合，以确保评估的全面性。
- **特征之间的关系**：相关性高的特征会相互影响彼此的SHAP值，例如两个高度相关的特征可能会分摊各自的贡献。
- **模型类型**：不同类型的模型（如决策树、神经网络等）对特征贡献的计算方式不同，从而影响SHAP值的结果。
- **特征重要性**：特定模型中更重要的特征通常会在SHAP值中获得更高的边际贡献分配。

**SHAP值的实际应用**

- **模型解释**：SHAP值帮助解释机器学习模型的决策过程，揭示每个特征对预测结果的影响程度，使得模型的预测结果更加透明。
- **奖金或资源分配**：在图示案例中，SHAP值用于公平地分配奖金或资源。不同特征的贡献量化后，可以用于制定奖励分配方案，确保分配的公平性。
- **领域专家的作用**：在一些应用场景中，领域专家的专业知识也可以纳入SHAP分析，以提供更加合理的解释和分配。

### 3.3.2 Shapley值计算

Shapley 值的计算公式如下：

$$
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(n - |S| - 1)!}{n!} \left[ v(S \cup \{i\}) - v(S) \right]
$$

其中：

- $$S$$ 为特征的子集；
- $$|S|$$ 为子集的大小；
- $$n$$ 为总特征数；
- $$v(S)$$ 表示特征子集 $$S$$ 对模型输出的贡献；
- $$\phi_i$$ 表示特征 $$i$$ 的 Shapley 值。

**边际贡献的计算**

Shapley 值通过计算边际贡献来衡量每个特征的影响：

- 边际贡献定义为 $$v(S \cup \{i\}) - v(S)$$，表示在子集 $$S$$ 的基础上加入特征 $$i$$ 后的贡献增量。
- 通过计算特征在不同组合中的边际贡献，评估每个特征对模型预测的具体影响。

**权重分配**

为了确保 Shapley 值的公平性，需要对每个子集的边际贡献进行加权，公式如下：

$$
\frac{|S|!(n - |S| - 1)!}{n!}
$$

这个权重确保了每种组合在计算过程中被公平地考虑。

**Shapley 值计算示例**

通过以下步骤计算每个特征的 Shapley 值：

- 计算特征在所有可能组合中的边际贡献。
- 加权平均每个特征的边际贡献，得出 Shapley 值。

例如，一个特征的边际贡献可能是 $$70\%$$、$$10\%$$、$$60\%$$，将这些值加权平均后，即为该特征的 Shapley 值。

**特征组合的排列和计算**

Shapley 值计算过程中涉及特征组合的排列，以确保计算的全面性：

- 当特征数量为 $$n = 4$$ 时，所有可能的子集组合数为 $$2^4 = 16$$。
- 每个特征的 Shapley 值通过在所有组合中的边际贡献加权求和得到。

**计算总结**

完整的 Shapley 值计算步骤为：

1. 遍历所有可能的特征组合。
2. 计算每个特征在不同组合中的边际贡献。
3. 对边际贡献加权，得到每个特征的 Shapley 值。
4. 将 Shapley 值应用于模型解释，以公平分配每个特征的贡献。

这种计算方法确保了每个特征贡献的公平分配，避免某一特征在模型中被高估或低估。

**总结**

- Shapley 值：提供了一种公平的方法来衡量每个特征对模型预测的贡献。
- 边际贡献：通过对特征子集的增量贡献进行计算，反映了特征在不同组合下的影响。
- 加权平均：确保所有组合的贡献被公平考虑，避免单一特征的影响被夸大或忽略。
- 排列组合：帮助遍历所有可能的特征组合，使得 Shapley 值的计算更加全面和准确。

### 3.3.3 Shapley values in XAI

**应用于机器学习**

- 将特征视为“玩家”（players）。
- 将模型行为视为“利润”（profit），例如预测结果、损失等。
- 使用 Shapley 值来量化每个特征的影响。

**SHAP**

- SHAP 是 Shapley Additive exPlanations 的缩写。
- SHAP 值在机器学习中的使用得到了推广，尤其在解释模型预测上非常有帮助。
- SHAP 使用 Shapley 值来解释个体预测结果。

**SHAP 作为基于移除的解释方法**

- 回顾基于移除解释的三个选择：
  1. **特征移除**：$$F(x_S) = E_{x_{\bar{S}}|x_S}[f(x_S, x_{\bar{S}})]$$
  2. **模型行为**：$$v(S) = F_y(x_S)$$
  3. **总结**：$$a_i = \phi_i(v)$$，其中 $$\phi_i(v)$$ 为 Shapley 值。

- 基于移除的方法通过逐步去除特征并计算其对模型输出的影响，来解释特征对预测结果的贡献。



## 3.4 Propagation-based explanations

### 3.4.1 Layerwise Relevance Propagation (LRP)

**Layer-wise Relevance Propagation (LRP)** 展示了逐层相关性传播方法的工作原理，用于解释深度学习模型的预测。LRP 是一种解释机器学习模型预测的方法，特别适用于深度神经网络，通过逐层传播相关性来识别哪些输入特征对最终的分类结果影响最大。以下是对图中内容的总结：

**输入图像**：图的左侧展示了一个输入图像，在示例中可能是一张包含动物或其他对象的图片。

**模型预测**：输入图像通过深度神经网络进行处理，模型输出对不同类别的预测概率。

**相关性传播过程**：LRP 从神经网络的输出层开始，将相关性分数逐层反向传播回去，一直到输入层。每一层中的每个神经元（或特征）都会根据其对输出的贡献获得一个相关性分数。

**逐层传播计算**：每一层的相关性分数会根据不同的规则进行分配，通常使用以下公式：
$$
R_j = \sum_i \frac{a_j w_{ji}}{\sum_j a_j w_{ji}} R_i
$$
其中，$$ R_j $$ 表示第 $$ j $$ 个神经元的相关性分数，$$ w_{ji} $$ 表示连接权重，$$ a_j $$ 表示第 $$ j $$ 个神经元的激活值。LRP 的目的是在各层之间传播相关性分数，最终在输入层生成一个相关性热图，展示对预测最有贡献的像素或特征。

**生成解释（相关性热图）**：通过逐层相关性传播，最终在输入图像上生成一个 **热图**，显示哪些区域对模型的预测结果贡献最大。热图中的红色区域表示对模型预测有正贡献的区域，蓝色区域表示负贡献区域。这样用户可以看到模型关注的图像部分，从而更好地理解预测背后的逻辑。

**主要优势**：直观可解释性：LRP 生成的热图使得用户可以直观地理解模型关注的区域。层级贡献分析：通过逐层传播，可以看到每层网络对最终结果的贡献，使得深度网络的决策过程更加透明。适用于深度神经网络：特别适合解释复杂的深度学习模型。

**应用场景**：LRP 常用于图像分类、对象识别等深度学习应用中，以帮助用户理解模型的预测依据。

---

- **直观性欠佳**：相比其他方法，LRP 的直观性较弱，并且需要一些启发式选择，例如选择哪种“规则”来分配相关性，这可能会让解释结果更加复杂。
- **适应性较差**：LRP 在不同的神经网络架构上可能难以适用，特别是在有不同结构的模型中。
- **示例**：例如，LRP 并不自动支持残差连接（如 ResNet 架构），在应用到变压器（transformer）模型时也需要进行扩展和修改。

<font color=blue>**例子：Layerwise Relevance Propagation (LRP) on MRI Data**</font>

图像展示了 LRP 应用于 MRI （脑肿瘤图像）脑部扫描图像，以解释深度学习模型如何识别不同类型的脑肿瘤。示例中的 MRI 图像包括 **胶质瘤**（Glioma）和 **脑膜瘤**（Meningioma）两种肿瘤类型。LRP 生成的热图展示了模型对不同肿瘤类型关注的区域，帮助解释模型的决策依据。

**数据集来源** 
数据集存储在 GitHub 仓库中，链接展示了脑肿瘤分类数据集的位置。数据集包含 MRI 图像，分为训练集和测试集，用于训练和评估分类模型。

**项目文件结构** 
展示了数据集的文件夹结构，包括 **glioma_tumor**、**meningioma_tumor** 等类别的文件夹，每个文件夹中包含对应类别的 MRI 图像。该结构有助于理解数据的组织方式，以便更方便地加载数据并用于模型训练。

**代码实现** 
代码主要展示了 LRP 方法的实现，代码文件位于 GitHub 中的 $$xai-series/05\_lrp.py$$。代码包含导入库、数据预处理、模型加载及 LRP 解释方法的实现。这些代码帮助将 LRP 应用到 MRI 图像分类任务中，以生成可解释性结果。

**代码运行界面** 
最后展示了在 IDE 中运行代码的界面，可以看到 LRP 解释生成的热图以及模型输出的结果。热图展示了模型在图像上关注的区域，使得用户可以直观地理解模型如何进行分类。

### 3.4.2 Gradient-based explanations

**Application to XAI** 

- **Idea**：找到在扰动时导致输出变化较大的特征。 
- **Remark**：该方法量化特征敏感性，但不一定与特征移除相关。

**Vanilla Gradients** 
对于输入 $$ x $$ 和标签 $$ y $$，计算预测 $$ f_y(x) $$ 的梯度：
$$
a_i = \frac{\partial f_y}{\partial x_i}(x)
$$
可以选择使用绝对值：
$$
a_i = \left|\frac{\partial f_y}{\partial x_i}(x)\right|
$$

**Variant 1: SmoothGrad** 
计算输入附近的梯度平均值。例如，添加高斯噪声：
$$
a_i = \mathbb{E}_{\epsilon}\left[\frac{\partial f_y}{\partial x_i}(x + \epsilon)\right] \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
$$
实际中，使用少量采样（50次），并需调节 $$ \sigma $$ 到适当水平。

**Variant 2: Integrated Gradient** 
梯度可能饱和，即使对重要输入也产生小梯度。模型对大输入变化敏感，但对小变化不敏感。 

- **Idea**：通过计算重缩放图像的梯度来解决饱和问题：

$$
x'(\alpha) = \bar{x} + \alpha(x - \bar{x}) \quad \text{for } 0 \leq \alpha \leq 1
$$

在重缩放图像范围内积分（平均）梯度：
$$
a_i = (x_i - \bar{x}_i) \int_0^1 \frac{\partial f_y(x'(\alpha))}{\partial x_i} d\alpha
$$

**GradCAM** 
在卷积神经网络（CNN）中，隐藏层表示高层视觉概念，隐藏层保留了因卷积结构而得的空间信息。 

- **Idea**：通过最后的卷积层而非输入层来解释模型。

**GradCAM Results** 
图示展示了 GradCAM 的结果。不同颜色表示模型关注的区域，从而直观显示模型决策时关注的图像部分。


### 3.4.3 Propagation vs. removal-based explanations

**Many explanation methods**  

- **Removal-based explanations**: 包括 SHAP、LIME、RISE、Occlusion、Permutation Tests。  
- **Propagation-based explanations**: 包括 SmoothGrad、IntGrad、GradCAM。  
- **实践选择**: 该使用哪种方法取决于特定需求和模型类型。

---

**Model flexibility**  

- **你在解释哪种模型？**  
  - **Removal-based explanations** 是模型无关的，可以应用于各种模型（如 CNNs、Trees 等）。  
  - **Propagation-based explanations** 主要用于神经网络，通常需要计算导数，有时还对模型架构有特定要求。

---

**Data flexibility**  

- **你有哪种类型的数据？**  
  - **Removal-based explanations** 可以处理离散和连续特征数据，例如，适用于数据集中具有不同值的缺失特征。  
  - **Propagation-based explanations** 更适合连续特征，能很好地捕捉输入特征的微小变化。

---

**Local or global**  

- **你需要哪种类型的解释？**  
  - 两种方法都可以生成局部解释。  
  - **Removal-based methods** 更适合全局解释，关注整体模型行为（如特征重要性）。  
  - 对于 **Propagation-based methods**，如果需要全局解释，则需将局部解释汇总。

---

**Speed**  

- **速度是否重要？**  
  - **Propagation-based methods** 更快，因为只需反向传播一次，与特征数量依赖性弱。  
  - **Removal-based methods** 通常较慢，因为需多次进行预测，尤其是 SHAP 等方法。

---

**Quality**  

- **哪个解释最具信息性或正确性？**  
  - 理论上可以作为指导，但也可以采取经验方法来衡量解释质量。  
  - **Perspective**: 无解释是完美的，但一些方法可能与用户问题不完全匹配。

---

**Popular methods**  

- **哪些方法最受欢迎？**  
  - 只有少数几种方法占主导地位，取决于数据领域（表格、图像、自然语言处理等）。

---

**Tabular data**  

- **Permutation tests** 广泛用于全局特征重要性。  
- **SHAP** 在局部解释中很流行，例如 **TreeSHAP** 内置于 XGBoost 和 LGBM 中，**KernelSHAP** 则用于其他模型。

---

**Computer vision**  

- **GradCAM** 和 **IntGrad** 在视觉领域最流行。  
- **Removal-based methods** 通常较慢。  
- 一些论文在尝试改进，但尚未流行。

---

**NLP**  

- 自然语言处理模型（如 LSTMs、Transformers）可以使用大多数解释方法。  
- **Gradient-based methods** 较流行，**Removal-based explanations** 较慢，但偶尔会用留一法（Occlusion）。  
- 对于 Transformers，可使用注意力作为解释。

---

**Popular packages** 
列出了一些流行的解释包，例如 **shap**、**lime**、**captum**、**innvestigate** 等，并展示了其 GitHub 星标数，说明其受欢迎程度。
