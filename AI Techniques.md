[TOC]



# 1 Introduction to AI

- 定义：A branch of computer science that deals with creating computer systems or software that can do tasks that usually require human intelligence.

- 图灵测试：图灵测试是一组指导原则，用于测试机器是否表现出类似于人类的智能行为。

- **AI分支**：

  - Machine Learning (ML)：Supervised，Unsupervised，Deep Learning

  - Natural Language Processing (NLP)：Content Extraction，Classification，Machine Translation，Question Answering，Text Generation

  - Expert Systems

  - Vision：Image Recognition，Machine Vision

  - Speech：Speech to Text，Text to Speech

  - Planning

  - Robotics

- 人和机器学习对比

| Category     | Details                                                      |
| ------------ | ------------------------------------------------------------ |
| Similarities | Learn, Experience, Feedback                                  |
| Differences  | General vs. specific learning, Small vs. big data, Learning speed |

- **Designing a Learning System**

  - Choosing Training Experience

  - Choosing Target Function

  - Choosing Representation of Target Function

  - Choosing Function Approximation

  - Final Design

- **ML Models**

  **Supervised Learning**

  - 从带标签的数据中学习。
  - 常用算法：决策树（Decision Tree）、支持向量机（Support Vector Machine）。
  - 示例应用：分类、回归等任务。

  **Unsupervised Learning**

  - 从无标签的数据中学习。
  - 常用算法：K均值聚类（K-Means Clustering）。
  - 示例应用：聚类、降维等任务。

  **Reinforcement Learning**

  - 通过持续的试错和奖励学习做出决策。
  - 常用方法：马尔可夫决策过程（Markov Decision Process）。
  - 示例应用：机器人控制、游戏策略等。

- **Types of Supervised Learning**

  - **Classification**
    - 用于预测类别型输出的问题，如正负分类、是否活体等。
    - 示例任务：
      - 图像分类（如医疗图像分类）
      - 文本分类（如情感分析、垃圾邮件检测）


  - **Regression**
    - 用于预测数值型输出的问题，如重量、预算等。
    - 示例任务：
      - 股票价格预测
      - 房价预测（基于面积）
      - 收入预测

- **Machine Learning vs Deep Learning**

  - **Machine Learning**：通过手动提取特征进行分类。例如，在图像识别中，需要人类定义特征（如边缘、形状等），然后机器学习算法进行分类。

  - **Deep Learning**：利用卷积神经网络（CNN）自动提取特征，减少人工干预。在图像识别中，深度学习模型能够从数据中直接学习特征，随着数据量的增加性能会显著提升。


- **Artificial Intelligence vs Machine Learning vs Deep Learning**

  - **Artificial Intelligence (AI)**：发展智能系统和机器，执行通常需要人类智能的任务。

  - **Machine Learning (ML)**：创建算法，从数据中学习并根据观察到的模式做出决策。当决策错误时可能需要人类干预。

  - **Deep Learning (DL)**：利用人工神经网络自动做出准确决策，通常不需要人类干预。
  - 关系AI包含ML，ML包含DL

- **Generative AI**
  - Generative AI models learn patterns from input data and then generate new content based on that learned information.
  - 模型使用神经网络和机器学习



# 2 INTELLIGENT AGENT

智能代理（Intelligent Agent）是一个自主实体，它通过传感器（sensors）感知环境（environment），并通过执行器（actuators）对环境进行行动（actions），以实现特定目标。

- 智能代理通过“感知-行动”循环来进行交互。它首先通过传感器接收来自环境的信息（称为感知 perceives），然后根据感知到的状态进行决策，最后通过执行器对环境产生影响。

<font color=blue>**Characteristics**</font>

| 特点                                 | 定义                                                         | 重要性                                                       | 示例                                                         |
| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 自主性 (Autonomy)                    | 智能代理无需人类或其他系统的直接干预即可独立运行             | 使系统能够独立决策，适用于需要实时响应且人工干预困难的应用   | 自主无人机避开障碍物，自动驾驶汽车换道决策                   |
| 社交能力 (Social Ability)            | 智能代理能够与其他代理、系统或人类有效互动                   | 使代理能够收集、共享和利用集体信息，对提升协作效率至关重要   | 聊天机器人参与人类式对话，多代理系统协同解决复杂问题         |
| 反应性 (Reactivity)                  | 智能代理能够实时感知并响应其环境                             | 使代理能够适应变化的情况，对安全关键应用至关重要             | 工业机器人应对意外障碍，语音助手响应语音指令                 |
| 前瞻性 (Proactiveness)               | 智能代理能够基于预测建模和对未来事件的预判主动采取行动       | 使代理具备前瞻性，提前规划行动，并在动态环境中具备竞争优势   | 智能温控器预测用户行为调整温度，交易机器人预测市场变化并预先交易 |
| 学习与适应 (Learning and Adaptation) | 智能代理能够通过从经验中学习来提升其表现，并根据情况调整行动 | 确保持续改进和优化，使代理在不断变化的情境中保持相关性和效率 | 根据用户推荐个性化内容，游戏中自适应AI根据玩家技能调整       |

<font color=blue>**Criterion to Design an Agent**</font>

**PEAS**: Performance, Environment, Actuators, Sensors

例子：

| Agent Type  | Performance Measure                                   | Environment                                  | Actuators                                           | Sensors                                                      |
| ----------- | ----------------------------------------------------- | -------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| Taxi Driver | Safe, fast, legal, comfortable trip, maximize profits | Roads, other traffic, pedestrians, customers | Steering, accelerator, brake, signal, horn, display | Cameras, sonar, speedometer, GPS, odometer, accelerometer, engine sensor, keyboard |

**Environment Type**

| 环境类型                              | 定义                                                         | 示例                                                 |
| ------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- |
| 完全可观察环境 (Fully Observable)     | 智能代理可以在任意时间点访问环境的所有状态和细节             | 国际象棋游戏，所有棋子的位置都对双方玩家可见         |
| 部分可观察环境 (Partially Observable) | 智能代理只能有限度地访问环境的状态或细节，部分信息可能是隐藏或未知的。 | 扑克牌游戏，玩家看不到其他玩家的手牌                 |
|                                       |                                                              |                                                      |
| 确定性环境 (Deterministic)            | 行动的结果是预定的和确定的                                   | 国际象棋，在相同状态和动作下，结果状态总是相同的     |
| 随机性环境 (Stochastic)               | 行动的结果具有概率性，可能会变化                             | 股票市场预测，因不可预见的因素，决策可能导致不同结果 |
|                                       |                                                              |                                                      |
| 回合式环境 (Episodic)                 | 智能代理在不同的回合中体验环境，每个回合是独立的             | 机器人吸尘器清扫房间                                 |
| 顺序式环境 (Sequential)               | 智能代理有连续的体验，决策具有长期影响                       | 自动驾驶车辆导航                                     |
|                                       |                                                              |                                                      |
| 静态环境 (Static)                     | 环境在智能代理采取行动之前保持不变                           | 数独游戏，棋盘在玩家行动前保持不变                   |
| 动态环境 (Dynamic)                    | 环境可以在智能代理思考时改变                                 | 股票市场交易，价格随外部因素波动                     |
|                                       |                                                              |                                                      |
| 离散环境 (Discrete)                   | 环境有有限的、独立的状态或结果                               | 国际象棋中，智能代理选择有限的合法移动               |
| 连续环境 (Continuous)                 | 环境有无限数量的状态                                         | 自动驾驶车辆导航交通                                 |
|                                       |                                                              |                                                      |
| 单代理环境 (Single-Agent)             | 只有一个智能代理进行操作和决策，其成功取决于自身行为         | 机器人吸尘器在房间中独立清洁                         |
| 多代理环境 (Multi-Agent)              | 多个智能代理同时运行，彼此可能协作或竞争                     | 多辆自动驾驶汽车在繁忙的十字路口导航                 |

<font color=blue>**Agent Types**</font>

| Agent Type               | 特点                                                   | 描述                                         | 示例应用                                     |
| ------------------------ | ------------------------------------------------------ | -------------------------------------------- | -------------------------------------------- |
| Simple Reflex Agent      | 基于条件-动作规则直接做出反应，不考虑环境的状态        | 只根据当前感知做出反应，没有记忆或状态       | 简单机器人吸尘器，在脏的时候清扫，干净时移动 |
| Model-Based Reflex Agent | 通过状态维护环境的模型，可以记住部分过去的信息         | 基于当前感知和内置的环境模型做出决策         | 可以记住房间状态的清扫机器人                 |
| Goal-Based Agent         | 具备目标导向，根据设定的目标选择行动                   | 为达到目标而行动，考虑长远的效果             | 导航系统，寻找从A点到B点的路径               |
| Utility-Based Agent      | 选择能带来最高效用的行动，评估行动带来的幸福感或满意度 | 考虑目标及其效用，基于效用值来决定行动       | 自动驾驶车辆选择最安全或最省油的路径         |
| Learning Agent           | 具备学习能力，通过反馈和经验不断提高                   | 包含学习和性能模块，可以根据环境反馈调整策略 | 智能助手，如Alexa，根据用户的偏好调整响应    |

<font color=blue>**Future of Agent**</font>

- 与物联网设备的集成
- 提升类人交互能力
- 人工智能代理的伦理考量
- 潜在挑战：安全性、保障性、可信度

<font color=blue>**总结**</font>

| Topic                        | Summary                                                      |
| ---------------------------- | ------------------------------------------------------------ |
| Nature of Intelligent Agents | 智能代理的本质是自主实体，能够感知环境、进行推理并做出决策，以实现特定目标。其应用范围从简单的自动化系统到复杂的AI驱动工具 |
| Architectural Diversity      | 介绍了不同类型智能代理的架构，从简单反射型代理到学习型代理，并分析了架构中增加的不同功能，以提高代理的性能 |
| Applications in Real-world   | 智能代理在日常生活中的广泛应用，从虚拟助手到推荐系统，突显了其在现代技术中的重要性 |
| Future Landscape             | 随着与物联网设备的深入集成及类人交互能力的追求，智能代理的未来充满机遇，但也面临挑战，如伦理对齐、安全性及可信度问题 |
| The Imperative to Understand | 随着数字与现实世界的界限逐渐模糊，理解智能代理的机制、潜力和风险变得至关重要。它们不仅能重塑行业、提升用户体验，还带来有关机器在我们生活中角色的哲学和伦理问题 |

# 3 Problem Solving by Searching

- **定义**：在AI中，问题求解是从起始状态移动到目标状态的过程。
- 多条路径可能通向目标，但并非所有路径都高效或可行。  一些问题可能无解。

- 目标导向型智能代理需要达成特定目标

**问题表述**  
- 状态空间：AI运行的环境，包含所有可能的状态
- 目标测试：确定某状态是否为目标状态
- 后继函数：提供从当前状态可能采取的所有行动
- 路径成本：从一个状态移动到另一个状态或达成目标的努力衡量

<font color=blue>**搜索算法类型**</font>

<font color = red>todo：复习每个搜索算法，给定一个图知道遍历顺序</font>

## 3.1 无信息搜索Uninformed/Blind Search

仅使用问题定义中的信息来进行搜索的策略。

- Breadth-first Search (BFS)
- Depth-first Search (DFS)
- Uniform-cost Search (UCS)
- Depth-limited Search
- Iterative Deepening Search
- Bidirectional Search

<font color=blue>**(1)广度优先搜索 (BFS)**</font>

**定义**：广度优先搜索是一种先探索当前深度所有节点，然后再移动到下一深度层的搜索策略

**特点**：

1. **完备性**：如果存在解，BFS一定会找到它
2. **最优性**：在均匀代价问题中，BFS能保证找到代价最小的解
3. **时间复杂度**：O(b^d)，其中b是分支因子，d是深度
4. **空间复杂度**：O(b^d)，因为需要存储当前层的所有节点以生成后继节点

**过程**：

- 使用队列（FIFO-先进先出）数据结构来存储节点。
- 选择图中的任意节点作为根节点，从该节点开始遍历。
- 遍历图中所有节点并逐个标记为完成。
- 访问相邻的未访问节点，标记为已访问，并将其插入队列。
- 如果没有相邻未访问节点，从队列中移除前一个节点。
- BFS算法会一直迭代，直到图中的所有节点都遍历并标记完成。
- 在遍历过程中不会出现环路。

<font color=blue>**(2)深度优先搜索 (DFS)**</font>

**概念**：递归算法，使用回溯来探索路径

**类比**：类似于解决迷宫问题，沿一条路径行进，直到遇到死胡同

**步骤**：

- 遇到死胡同时，返回到上一个未尝试的路径
- 如果没有前进的路径，则回退
- 在切换路径前，会先彻底探索当前路径上的所有节点
- 目标：每个节点都访问一次
- 关键思想：尽可能深入，再回溯

<font color=blue>**(3)深度限制搜索 (Depth-Limited Search, DLS)**</font>

**定义**：深度限制搜索（DLS）是深度优先搜索（DFS）的一个变种。

**特点**：不同于DFS，DLS设置了一个预定义的深度限制，防止陷入无限循环或过深的搜索。

**终止条件**：DLS有两个失败终止条件：

1. **标准失败值** (Standard failure value) ：当搜索无法在给定的深度限制内找到目标节点，且没有其他路径可供探索时，返回标准失败值。也就是说，在当前深度限制下搜索已经完成，但未能找到解决方案。
2. **截止失败值** (Cutoff failure value)：当搜索到达了深度限制，但仍可能存在更深层的路径时，返回截止失败值。这意味着搜索并未真正失败，而是被深度限制终止。可以通过增加深度限制来继续探索更深的路径。

<font color=blue>**(4)迭代加深搜索 (Iterative Deepening Search, IDS)**</font>

**定义**：迭代加深算法是深度优先搜索（DFS）和广度优先搜索（BFS）的一种组合

**过程**：

- 逐步增加搜索深度限制，直到找到目标，从而找到最佳的深度限制
- 每次迭代执行到特定的“深度限制”，在目标节点未找到时继续增加深度限制

**优点**：当搜索空间较大且目标节点深度未知时，IDS是非常有用的无信息搜索策略

<font color=blue>**(5)一致代价搜索 (Uniform Cost Search, UCS)**</font>

- 用于遍历加权树或图
- 探索累积代价最低的路径
- 使用优先队列实现，以最低代价为优先
- 当边有不同代价时启用
- 目标是找到到达目标节点的最小代价路径
- 如果所有边的代价相同，则等同于广度优先搜索



## 3.2 有信息搜索Informed/Heuristic Search

**智能搜索和场景背景**（**Intelligent Search and Scenario Context**）：智能搜索旨在通过避免穷举路径来有效地寻找解决方案，但在复杂场景中，有时可能会陷入死胡同。

**启发式搜索**（**Heuristic Search**）：此方法利用**启发式（经验法则）**来指导搜索过程，帮助跳过不相关区域，专注于可能的路径。然而，启发式搜索并不保证找到解决方案，可能会卡住。

**改进盲目搜索策略**（**Improving Blind Search Strategy**）：启发式函数用于避免盲目搜索的低效率，使搜索过程更加高效。目标是减少不必要的探索，从而节省时间和计算资源。

**选择一个好的启发式**（**Choosing a Good Heuristic**）：并非所有启发式方法都同样有效。一个好的启发式可以最小化搜索中所需的节点数量，从而提高搜索效率。提供更好指引的启发式被称为“信息更充分”的启发式。

<font color=blue>**(1)Best First Search**</font>

最佳优先搜索也称为贪心最佳优先搜索，通过启发式函数估算每个节点的代价，并选择看起来最接近目标的节点。

- 最佳优先搜索的核心思路是每次选择看起来离目标节点最近的节点，从而避免全面搜索。

**代价函数 f(n)**：评估每个节点的代价，通过代价函数f(n)来选择节点。

- 实际上，代价函数f(n)是一个启发式函数h(n)。
- h(n) 表示从节点 n 到目标节点的估计代价。
- f(n) 表示通过节点 n 到目标节点的总估计代价。

**节点选择**：选择估计代价最低的节点进行探索。

**操作步骤**：

1. 从初始节点开始。
2. 持续选择估计代价最低的节点。
3. 当到达目标节点时停止搜索。

<font color=blue>**A* Search**</font>

A* 是一种领先的路径搜索算法，用于找到从起点到目标的最优路径。其核心思想是选择看起来最有希望的节点进行探索，依据一个代价函数 $$f(n)$$ 来评估每个节点。

- A* 搜索算法通过结合实际代价 $$g(n)$$ 和启发式估计 $$h(n)$$，能有效地找到从起点到目标的最优路径。它利用开放和封闭列表来管理节点的状态，确保最小化搜索路径。

**代价函数：**$$f(n) = g(n) + h(n)$$

- $$g(n)$$：从起点到节点 $$n$$ 的实际代价。
- $$h(n)$$：从节点 $$n$$ 到目标的估计代价（启发式）。
- $$f(n)$$：从起点通过节点 $$n$$ 到目标的总估计代价。

**节点选择**：选择当前代价 $$f(n)$$ 最低的节点来继续探索。

**操作步骤**

1. 维护两个列表：
   - 开放节点（Open Nodes）：存放已发现但尚未评估的节点。
   - 封闭节点（Closed Nodes）：存放所有已评估的节点。(直线距离肯定比实际距离短,所以后面出现了实际加起来比前面直线算的要短的，可以封闭节点)
2. 初始节点：从初始节点开始。
3. 发现和评估节点：每次从开放节点中选择代价最低的节点进行探索。
4. 移动节点：
   - 探索后，节点进入开放列表。
   - 评估完成后，节点从开放列表移到封闭列表。
5. 终止条件：当目标节点成为当前代价最低的节点时，搜索结束。

**A* 搜索算法的性质**

- 完备性？是的（除非存在无限多个满足 $$f \leq f(G)$$ 的节点）
- 时间复杂度？指数级
- 空间复杂度？需要将所有节点保存在内存中
- 最优性？是的

<font color=blue>**极小极大算法（Minimax Algorithm）**</font>

- 极小极大算法是一种**回溯算法**，用于在决策和博弈论中帮助玩家找到最佳行动。
- 通常应用于**双人回合制游戏**，如井字棋（Tic-Tac-Toe）、西洋双陆棋（Backgammon）、曼卡拉（Mancala）、国际象棋（Chess）等。
- 在算法中，有两种角色的玩家：
  - 极大化者（Maximizer）：目标是获得最高得分
  - 极小化者（Minimizer）：目标是获得最低得分

在这个算法中，轮到最小化玩家选择的时候，最小化玩家会选择那个能够导致最大化玩家得分最低的节点。一旦最小化玩家做出选择，那个选择的分值就成为当前路径的最终得分，供最大化玩家在根节点做决定时参考。

<font color=blue>**Alpha-Beta 剪枝算法**</font>

在 Alpha-Beta 剪枝算法中：

- **最大化玩家的目标**是获得尽可能高的分数。
- **最小化玩家的目标**是让最大化玩家获得尽可能低的分数。

所以在决策树中，最大化和最小化节点的角色会交替进行，每个节点都在试图为自己争取最佳结果。

**何时剪枝？**

1. **定义**：
   - **α** 是当前已知的 **最大化玩家**可以确保的最低得分（在最大化节点上逐步更新）。
   - **β** 是当前已知的 **最小化玩家**可以确保的最高得分（在最小化节点上逐步更新）。
2. **场景描述**：
   - 假设我们在一个 **最小化节点**上，已经有一个分支得到了值 V，并且V≤α。
   - 这个值 V 表示的是这个分支的最小得分。而 **最大化玩家的目标是获得比 α 更高的得分**（因为 α 是在之前的最大化节点上选择的最佳分数下界）。
3. **推理过程**：
   - 如果当前分支的得分 V≤α，这意味着 **最小化玩家可以通过选择这个分支，让最大化玩家的得分小于或等于 α。
   - **对于最大化玩家来说，这是一个不可接受的选择**，因为他可以选择其他分支获得更高的得分（至少是 α）。
4. **结论**：在这种情况下，**我们可以安全地停止探索该分支的其他可能性**。即便后续的子节点可能存在更低的分值，最大化玩家都不会选择这一条路径。