[TOC]



# 1 入门

**1.传统算法和机器学习算法比较：**

- **传统算法：** 基于手写规则的程序，如能获得优惠计划的客户分类
- **机器学习算法：** 无需简单规则，基于多个输入数据自动学习，如贷款风险评估

**2.机器学习的基本成分与模型训练**

- **函数类:** 定义输出 (y) 与输入 (x) 的关系

- **训练数据:** 包含特征和标签的数据样本

**3.现代机器学习的黑盒问题**

- **黑盒模型：** 尽管模型的准确性与商业收入、用户体验密切相关，但缺乏透明度

- **透明性的重要性：** 需要识别关键因素并生成科学假设

**4.临床预测的关键问题：Why**

- **挑战：** 精确预测临床结果很重要，但核心问题在于理解预测背后的原因

**5.解释性AI在生物学和健康中的应用**

- **解释性的重要性：** 解释性有时与准确性同等重要，例如在临床应用中，了解特征对预测的影响至关重要

- **关键问题：**

  - 哪些特征对某一预测起到了关键作用？

  - 如何选择或学习最具解释性或信息量的特征？

  - 如何使黑盒模型具有生物学或临床意义？

**6.黑盒模型的透明性目标**

- 目标包括：
  - 识别潜在过程中的关键因素
  - 生成科学假设
  - 模型故障诊断和审查
  - 审核不必要的依赖项
  - 改善数据集和提高信任

**7.解释性AI的关键成分**

- **模型：** 可以是黑盒模型 (如DNN, GBM)
- **数据：** 单个数据样本或整个数据集
- **问题：** 需要了解什么方面？

**8.XAI 问题类型**

- **特征重要性**：哪些特征对预测最重要？
- **概念重要性**：模型理解的关键概念是什么？
- **数据实例重要性**：每个样本对模型训练的贡献程度

**9.临床预测中的准确性与解释性**

- 尽管准确预测临床结果很重要，但关键问题在于“为什么”模型做出这样的预测。
- **精确性与可解释性：** 简单模型（如线性模型）通常解释性强，但性能差；复杂模型（如黑盒模型）尽管准确，但难以解释。
- **SHAP方法：** Scott Lundberg 和 Su-In Lee提出了SHAP方法，用于任何模型的特征重要性计算，能解释单一预测的依据。
- 临床预测中的解释性提高了决策支持：使用XAI提高手术期间低氧血症的预测准确性，提供实时解释，帮助麻醉师识别潜在的低氧风险。

**10.解释性AI促进模型审核和成本意识**

- **模型审核的必要性：** 许多基于AI的COVID-19检测系统依赖于“捷径”特征而非真正的病理学依据，表明模型审核的重要性。

- **成本意识AI (`Cost-Aware AI`,CoAI)：** CoAI方法显著减少特征获取成本（如时间），可应用于急诊和重症监护。

  

# 2 什么是XAI

**1.解释性AI (XAI) 的定义**

- **狭义定义：** 让模型决策对人类更易理解的方法和技术
- **广义定义：** 让所有AI相关内容（如数据、功能、性能等）更易理解
- **重点：** 主要关注监督式机器学习的解释
- **权衡问题(Trade-Off Issue)：** 高性能模型（如深度神经网络）往往难以解释，直接可解释模型性能相对较低。
- **XAI重要性：** 解释性是负责任AI的基础，有助于实现公平性、可用性、安全性等目标。
  - **解释性调试：** XAI 有助于识别模型中的问题并通过调试改进模型。
  - Kulesza 等人提出的解释性调试方法，可以通过用户反馈不断优化模型

**2.监督式机器学习**

- **学习模型：** 使用算法训练模型，以预测新实例的标签。
- **特征解释：** XAI的重点是解释模型的决策过程，例如苹果蛋糕的颜色、形状和气味等特征。
- **模型解释：** 包括性能、限制、输出等。

**3.XAI类型**

- **直接解释模型**：如线性模型、决策树、基于规则的模型
- **事后解释模型**：针对黑盒模型，如深度神经网络，可用近似方法提供解释。

**案例：贷款申请审批中的决策支持**

- **参与角色：** 数据科学家、贷款官员、客户
- **需求：** 客户希望了解贷款审批的理由，贷款官员需要对预测结果有信心。

**4.模型的全局与局部解释**

- **Knowledge Distillation**：

  - 知识蒸馏是一种方法，通常用来将复杂模型（如深度神经网络）的知识“蒸馏”到一个更简单的模型中。

  - 在这里，知识蒸馏被用来将黑盒模型的行为通过近似方法转移到一个更易解释的模型上，例如决策树或线性模型，以便更好地理解原始模型的整体行为。

- **全局解释：** 如决策树近似，帮助数据科学家理解整体模型的决策逻辑。
- **局部解释：** 分析单个预测，如局部特征贡献和原型示例，帮助贷款官员做出判断。

**5.事后解释算法示例**

**LIME方法：** LIME 是一种用于解释黑盒模型（如深度神经网络和随机森林）的局部解释方法。它通过构建一个 **易解释的、线性的近似模型** 来模拟黑盒模型在特定输入附近的行为，进而揭示该输入如何影响模型的预测。

<font color=blue>**步骤：**</font>

**生成扰动数据**：

- LIME 生成与输入样本相似的扰动数据集（例如，通过小幅改变特征值），并将这些数据输入到原始黑盒模型中，记录模型的输出。

**权重分配**：

- 根据扰动数据点与原始输入样本的相似性，LIME 为这些数据点分配权重，确保越接近原始样本的扰动点在解释中权重越高。

**训练局部线性模型**：

- 使用带权重的扰动数据，LIME 训练一个简单的线性模型来近似原始模型在该局部区域的行为。

**生成解释**：

- LIME 的最终输出是每个特征对该预测结果的贡献度（正或负），以揭示哪些特征在多大程度上影响了该预测。

**6.反事实检查**

- **反事实特征检查：** 如部分依赖图和对比特征，展示不同决策条件下的特征影响。
- **反事实示例：** 分析类似实例，如其他按时还款的客户，帮助理解贷款被拒的原因。

**7.XAI评价指标**

- **固有指标：** 如保真度、稳定性、简洁性
- **用户依赖指标：** 如可理解性、用户满意度
- **任务导向指标：** 任务表现和用户信任度

**8.不想要的偏差(unwanted bias)**

- **偏差问题：** 偏差可能会导致某些非特权群体处于系统性不利地位，在某些背景下甚至是非法的。
  - **背景：** COMPAS 系统用于预测犯罪率，但被发现对某些群体存在歧视性偏见。

**XAI审查**：

- **作用：** XAI 可以作为一种工具，用于分析和揭示模型中的歧视性偏见。

- **方法：** 通过对比性示例和特征重要性分析帮助用户理解模型决策

**9.XAI优点**

- **重要性：** XAI 提供的解释有助于用户理解决策背后的原因，从而采取合适的行动。
- **信任校准：** XAI 可以帮助校准人们对 AI 的信任，增加用户对系统的依赖度。
- **AI 文档和治理的趋势：** AI 的文件记录和问责机制变得越来越重要，像 IBM 的 FactSheets 和 Google 的 Model Cards 等工具已经被提出。
  - **目标：** 这些工具有助于在 AI 系统中实现透明性和可追溯性，保障 AI 的安全性和公正性

# 3 解释方法

## 3.1 特征重要性解释(Feature Importance Explanations)

**目的**：解释每个特征如何影响模型，并回答模型决策相关的问题。

**例子**：

- 包括自然图像、医疗图像、表格数据和时间序列等场景。
- 研究例子：Fong 和 Vedaldi 的研究“通过有意义的扰动解释黑盒模型”
  - 通过对图像进行有意义的扰动（例如遮盖或模糊特定区域），观察模型的预测变化。该方法旨在发现哪些图像区域对于模型预测最重要。
- 研究例子：Sundararajan 等人提出的“深度网络的公理化归因”
  - 提出了一种基于深度神经网络的归因方法，称为“积分梯度法”（Integrated Gradients），它利用数学公理来确定每个像素对最终预测的贡献。这种方法在不改变模型结构的情况下生成解释性信息。
- 研究例子：Lundberg 等人关于“树的解释性 AI：从局部解释到全局理解”
  - 提出了 SHAP（Shapley Additive Explanations）方法，基于博弈论中的 Shapley 值来计算每个特征在特定预测中的边际贡献。该方法可以为树模型提供局部和全局的解释。
- 研究例子：Covert 等人关于“用于自动癫痫检测的时间图卷积网络”（2019年）
  - 提出了时间图卷积网络（Temporal Graph Convolutional Networks），在时间序列中检测出癫痫发作等重要事件，并解释模型如何识别时间序列中的关键模式。

### 3.1.1 符号说明

常用的符号：

1. **𝑥**：输入数据或特征向量

   这是模型接受的输入，表示一个具体的数据实例。𝑥 包含了多个特征的值。

2. **𝑦**：模型的输出或预测结果

   𝑦 是模型对输入𝑥的预测输出，例如在分类问题中，𝑦 可能是一个类别标签。

3. **𝑓(𝑥)**：机器学习模型

   表示机器学习模型，它根据输入𝑥 生成输出𝑦。𝑓(𝑥) 的结构和算法因模型类型而异，如神经网络、决策树等。

4. **𝑥₁, 𝑥₂, ..., 𝑥𝑑**：特征（Features）

   每个输入𝑥包含𝑑个特征（feature），即𝑥是一个包含𝑑个维度的向量。例如，𝑥₁可以代表年龄，𝑥₂可以代表收入。

5. **𝑑**：特征的总数

   表示输入数据中所包含的特征数量。

6. **𝑛**：样本数

   数据集中数据点的总数，即样本数。𝑛个数据点表示模型接受了多少个独立的输入数据实例进行训练或测试。

### 3.1.2 定义

**模型解释（Model Explanation）**

模型解释的目标是突出模型为什么会做出某个预测。通过揭示模型在做出特定决策时的原因，帮助用户理解模型的行为。

**特征重要性解释（Feature Importance Explanation）**

- 特征重要性解释专注于每个特征的作用，展示各个特征对模型预测的影响大小。
- 这种解释可以是针对单个预测的（局部解释，Local）或针对模型整体行为的（全局解释，Global）。

**局部解释与全局解释**

- **局部解释（Local）**：局部解释关注单个数据实例的特征重要性，展示某个特定预测的原因。
- **全局解释（Global）**：全局解释旨在描述模型整体的行为，展示模型在所有数据点上的特征重要性。

**特征重要性解释的两种类型**

- **特征归因（Feature Attribution）**：对每个特征 $$ x_i $$ 赋予一个分数 $$ a_i $$，这个分数表示该特征对模型预测的贡献。
- **特征选择（Feature Selection）**：从所有特征中选择一个重要特征子集$$ x_S \subset \{x_1, x_2, \dots, x_d\} $$，这些特征在预测中起到关键作用。

**解释算法（Explanation Algorithm）**

解释算法是一种基于输入数据和机器学习模型生成解释的方法。它的目的是使模型的预测过程对用户更加透明。



## 3.2 基于移除的解释(Removal-based Explanations)

- **基本思想**：移除特征并观察预测变化，判断特征重要性。

- **医生类比与机器学习转化**
  - 通过遮挡医疗图像的部分区域，观察诊断变化，类似于 ML 中移除特征。

### 3.2.1 案例研究

#### 3.2.1.1 置换测试(permutation test)

置换测试是一种用于评估输入特征重要性的“老方法”，最早在随机森林模型中应用。通过该方法，可以确定每个输入特征的总体（全局）重要性。

- **步骤 1**：首先使用原始数据评估模型的准确性。
- **步骤 2**：逐个特征进行破坏，即随机打乱数据集中特定特征列的值（对应于该特征）。
- **步骤 3**：记录模型准确性的下降幅度，以此来衡量特征的重要性。

该方法通过对数据集中的每个特征进行随机扰动，观察其对预测精度的影响，从而评估各个特征的重要性。这就是置换测试的基本原理。

- 置换测试适用于任何模型
- 可用于连续特征或类别特征
- 快速且易于实现

**数学定义**

(1)直观的数学定义
$$
a_i = Acc(\text{original}) - Acc(x_i \text{ corrupted})
$$

- **$$ a_i $$**：特征 $$ x_i $$ 的重要性分数，表示当特征 $$ x_i $$ 被破坏或置换时，模型准确性的下降程度。
- **$$ Acc(\text{original}) $$**：模型在使用完整原始数据时的准确性。
- **$$ Acc(x_i \text{ corrupted}) $$**：当特征 $$ x_i $$ 被随机打乱或破坏后，模型的准确性。

(2)详细视角的数学定义
$$
a_i = \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., \tilde{x}_i^j, ..., x_d^j), y^j) - \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., x_i^j, ..., x_d^j), y^j)
$$

- **$$ a_i $$**：特征 $$ x_i $$ 的重要性分数，衡量当特征 $$ x_i $$ 被扰乱后，对模型损失的平均影响。
- **$$ n $$**：样本数量。
- **$$ f(x_1^j, ..., x_d^j) $$**：模型 $$ f $$ 的输出，基于输入特征 $$ x_1^j, ..., x_d^j $$。
- **$$ \ell $$**：任意损失函数（例如均方误差或交叉熵），用于衡量模型预测值和真实值 $$ y^j $$ 之间的误差。
- **$$ \tilde{x}_i^j $$**：特征 $$ x_i $$ 在第 $$ j $$ 个样本中的扰乱版本（例如被随机打乱的值）。
- **第一部分** $$ \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., \tilde{x}_i^j, ..., x_d^j), y^j) $$：表示破坏特征 $$ x_i $$ 后的平均损失。
- **第二部分** $$ \frac{1}{n} \sum_{j=1}^{n} \ell(f(x_1^j, ..., x_i^j, ..., x_d^j), y^j) $$：表示原始数据下的平均损失（即特征未被破坏的情况）。



#### 3.2.1.2 遮挡(Occlusion)

遮挡方法是一种早期用于深度学习模型的解释技术，主要用于图像分类任务中的个别预测解释。

- **遮挡方法适用于任何模型**：不仅限于图像数据，还可用于非图像数据的模型解释。
- **速度适中**：对每个预测进行解释时，需要进行 d+1d + 1d+1 次模型评估（其中 ddd 为特征数）。
- **简单易实现**：遮挡方法实现简便，适合各种应用场景。

**方法概述**

- **深度学习的早期方法**：遮挡方法在深度学习刚兴起时被提出，作为理解复杂图像分类模型的技术。
- **用于解释单个预测**：主要关注于解释模型如何对单个图像做出预测，帮助理解模型为何对特定输入图像做出特定预测。
- **计算像素（或超像素）的重要性**：通过逐步遮挡图像中的某些像素或超像素区域，观察模型预测结果的变化，从而计算这些区域对预测结果的重要性。

遮挡方法的具体过程如下：

1. **使用完整图像进行预测**：首先，模型在未遮挡的完整图像上进行预测，得到基线预测结果。
2. **遮挡图像的不同区域**：将图像的不同区域逐一遮挡，并记录每次遮挡后预测结果的变化。这个过程帮助识别出哪些区域对模型预测最为关键。
3. **用无信息（如零）像素替换遮挡区域**：遮挡通常通过将像素替换为无信息的值（如零）来实现，这样可以保证该区域对预测不再有任何贡献。
4. **遮挡的粒度**：遮挡操作可以在不同的粒度上进行，例如 2x2 或 4x4 的超像素（superpixel）块。较小的遮挡块可以获得更精细的解释，而较大的遮挡块则有助于快速识别重要区域。

**数学定义**

(1)直观视角的数学定义
$$
a_i = f_y(x) - f_y(x_{(-i)})
$$

- **$$ a_i $$**：特征 $$ a_i $$ 的重要性分数，通过计算完整输入 $$ x $$ 的模型输出 $$ f_y(x) $$ 与去掉第 $$ i $$ 个特征后的模型输出 $$ f_y(x_{(-i)}) $$ 之间的差值来得出。
- **目的**：通过遮挡特定特征，观察模型输出的变化，从而量化该特征对模型预测的重要性。

(2)详细视角的数学定义
$$
a_i = f_y(x_1, ..., x_d) - f_y(x_1, ..., 0, ..., x_d)
$$

- **$$ a_i $$**：特征 $$ a_i $$ 的重要性分数，计算为完整输入 $$ (x_1, ..., x_d) $$ 的模型输出与将第 $$ i $$ 个特征替换为零后的模型输出之间的差值。
- **替换为零**：在详细视角中，遮挡操作通过将特征 $$ x_i $$ 替换为零（或其他无信息的值）实现，使其对模型预测不再产生影响。

#### 3.2.1.3 置换和遮挡方式对比

- **为不同模型设计**：置换适用于随机森林，遮挡适用于卷积神经网络（CNN）

- **全局与局部解释**：置换适用于全局解释，遮挡适用于局部解释。

- **相似之处**：尽管方法不同，但这些方法之间存在一些显著的相似之处。

|              | 置换测试         | 遮挡           |
| ------------ | ---------------- | -------------- |
| 破坏输入     | 随机化特征       | 设为零         |
| 观察模型变化 | 观察准确性的变化 | 观察预测的变化 |
| 计算影响     | 移除单个特征     | 移除单个特征   |

### 3.2.2 基于移除解释方法的统一框架

想法：通过更改实现选项来创建新的解释方法。

**三种设计选择**：**特征移除方法**、**模型行为**和**汇总技术**

<font color = blue>**特征移除**</font>

- **核心思想**：模型通常需要所有特征才能进行预测，但我们希望从某些特征中移除信息。
- **特征移除的模拟**：大多数模型不支持直接移除特征，因此需要模拟特征移除。
- **实现方式**：
  - **使用默认值（如零）替换**：将要移除的特征替换为零值。
  - **使用随机值替换**：用随机值替换特征内容。
  - **为每个特征集训练单独的模型**：构建包含不同特征的独立模型。
  - **使用支持缺失特征的模型**：选择可以接受缺失特征的模型。
  - **模糊处理（针对图像）**：对图像中的区域进行模糊处理，模拟特征移除。

<font color = blue>**模型行为**</font>

- **观察模型行为**：可以移除特征并观察其对模型的影响。

- 选择观察的量

  需要选择一个具体的量来评估模型行为，例如预测值、预测损失或数据集损失。

  - **Prediction（预测）**：直接观察预测结果。
  - **Prediction loss（预测损失）**：观察预测结果的损失值。
  - **Dataset loss（数据集损失）**：整体数据集上的损失值 $$ E[\ell (y, \hat{y})] $$。

<font color = blue>**汇总技术**</font>

- **选择汇总方法**：可以使用任意特征子集来观察模型行为。
- **组合过多问题**：给定 d 个特征，有 $$ 2^d $$ 个子集需要考虑。如何有效地传达信息是一个挑战。
- **常见的汇总类型**：
  - **Feature selection（特征选择）**：选取一组重要特征的子集。
  - **Feature attribution（特征归因）**：分配特征分数，例如使用置换测试和遮挡法。

----------

<font color=blue>**常用特征移除方法**</font>

- **PredDiff**：通过条件期望生成解释和交互，使用条件填充模型来删除信息
  - 条件删除模型会删除某些特征，但通过填充与邻近值或上下文相关的内容，尽可能保持数据的“合理性”或“自然性”。
- **Meaningful Perturbations**：考虑多种从输入图像中删除信息的方法，推荐的操作是模糊化

<font color=blue>**常用汇总方法**</font>

**(1)RISE**：对缺失特征的多个子集进行采样，计算包含 $$ x_i $$ 时的平均预测

**(2)LIME**：对特征子集应用加权核 $$ \pi(S) $$，拟合线性/加性代理模型
$$
\min_{a_0, ..., a_d} \sum_{S \subseteq D} \pi(S) \left( a_0 + \sum_{i \in S} a_i - f_y(x_S) \right)^2 + \Omega(a_1, ..., a_d)
$$

- **目标**：LIME 通过拟合一个 **加性近似模型**，来解释复杂模型的预测。LIME 的目标是找到一组系数 $$ a_0, a_1, ..., a_d $$，使得这些系数尽可能地描述目标模型在局部区域的行为。
  
- **加性近似（Additive Approximation）**：公式的核心部分 $$ a_0 + \sum_{i \in S} a_i $$ 表示一个线性模型的近似，其中 $$ a_0 $$ 是偏置项，$$ a_i $$ 是每个特征的贡献分数。

- **权重函数 $$ \pi(S) $$**：用于衡量不同特征子集的重要性，通常根据特征子集与被解释数据点的相似性进行加权，确保越接近原始实例的特征组合在解释中权重越高。

- **正则化项 $$ \Omega(a_1, ..., a_d) $$**：这是一个可选的正则化项（如 Lasso），用于控制系数的稀疏性，帮助简化模型，使解释更易于理解。

**工作流程部分**

1. **模型输入和预测**：
   
   复杂模型（例如神经网络）接受数据输入并生成预测结果。这里的数据包括了多个特征，例如打喷嚏、体重、头痛等，模型预测疾病为“流感”（Flu）。

2. **解释器（LIME）**：
   
   LIME 解释器接收原始数据和模型预测，选择一组与预测相关的重要特征，通过线性近似来解释模型的局部行为。LIME 提取了特征“打喷嚏”、“头痛”和“无疲劳”作为解释特征，用来简化复杂模型的输出。

3. **人类决策**：
   
   最终，解释器生成的解释（如“打喷嚏”、“头痛”、“无疲劳”）被展示给用户，人类基于这些解释做出进一步的判断或决策。

### 3.2.3 Meaningful perturbations(扩展学习)

- **目的**：从模型正确分类的图像开始，通过对图像进行模糊处理来改变预测结果。

- **过程**：通过模糊操作逐渐改变图像，使得模型的预测结果发生变化。
- **示例**：对比模糊图像和原始图像，展示了模糊如何影响模型的分类准确性。

<font color = blue>**步骤**</font>

- 设 $$ x \in \mathbb{R}^{w \times h} $$ 为图像
- 设 $$ m \in [0, 1]^{w \times h} $$ 为遮罩
- 设 $$ \Phi(x, m) $$ 为遮罩图像

**(1)问题**：如何进行遮罩操作？

- **使用常量值替换**：

$$
\Phi(x, m)_{ij} = m_{ij} \cdot x_{ij} + (1 - m_{ij}) \cdot \mu
$$

- **使用噪声替换**：

$$
\Phi(x, m)_{ij} = m_{ij} \cdot x_{ij} + (1 - m_{ij}) \cdot \epsilon_{ij}
$$

其中 $$ \epsilon \sim \mathcal{N}(0, \sigma^2) $$

- **使用高斯核模糊**：

$$
\Phi(x, m)_{ij} = \text{blur with kernel } g_{\sigma}
$$

**(2)学习最优模糊**

- **初始状态**：目标类别 $$ y $$ 对 $$ \Phi(x, 1) $$ 的预测概率接近 1
- **目标**：学习一个遮罩 $$ m $$，使得 $$ f_y(\Phi(x, m)) \approx 0 $$
- **最小化以下损失**：

   $$
   \min_m f_y(\Phi(x, m))
   $$

**(3)其他考虑因素**

1. 模糊应尽量最小
2. 遮罩应平滑。
3. 优化应对抗对抗性扰动。

**实际损失函数**
$$
\min_m \mathbb{E}_{x} [f_y(\Phi(x, m))] + \lambda_1 \|1 - m\|_1 + \lambda_2 \| \nabla m \|_2^2
$$

- 损失项
  - $$ f_y(\Phi(x, m)) $$：预测误差项
  - $$ \|1 - m\|_1 $$：稀疏遮罩正则化
  - $$ \| \nabla m \|_2^2 $$：平滑正则化

**(4)优化**

- 实际损失函数：
   $$
   L(m) = \mathbb{E}_{x}[f_y(\Phi(x, m))] + \lambda_1 \|1 - m\|_1 + \lambda_2 \| \nabla m \|_2^2
   $$
   
- 通过随机梯度下降（SGD）确定最优遮罩：

   $$
   m^{(t+1)} = m^{(t)} - \alpha \frac{\partial L}{\partial m^{(t)}}
   $$

**(5)结果**

- **不同模糊处理效果的比较**：展示了在模糊、常量替换和噪声扰动下的遮罩效果。
- **图像示例**：从不同的遮罩和扰动类型中可以看出，模糊处理能够有效地影响模型对目标的注意区域





## 3.3 Shapley 值，涉及博弈论背景





## 3.4 基于传播的解释，分析模型对小变化的敏感度